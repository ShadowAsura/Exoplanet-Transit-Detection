# Overview 

Machine learning is a field of study that focuses on developing algorithms and techniques to enable computers to learn from data and make predictions or decisions without being explicitly programmed.

## Basic Concepts

### [[Supervised Learning]]
Supervised learning is a machine learning archetype where an  algorithm learns from labeled training data, where the input features and corresponding target labels are provided. Usually the goal is to learn a mapping between the input features and the target labels to make predictions on unseen data.

### [[Unsupervised Learning]]
Unsupervised learning involves learning patterns or structures in unlabeled data. Unlike supervised learning, there are no specific target labels to guide the learning process. Instead, the algorithm aims to identify inherent patterns, clusters, or relationships within the data.

### [[Feature Extraction]]
Feature extraction is the process of selecting or transforming raw input data into a more suitable representation that captures the essential information for a given learning task. It helps to reduce the dimensionality of the data, enhance relevant features, and improve the performance of machine learning models. ex. overfitting

### [[Model Training and Evaluation]]
Model training involves feeding the algorithm with labeled training data to learn patterns and build a predictive model. Evaluation is performed to assess the performance and generalization capability of the trained model using validation or test data. Various metrics, such as accuracy, precision, recall, and F1 score, can be used to evaluate the model's effectiveness.

## [[Machine Learning Algorithms]]

### Decision Trees
Decision trees are a widely used algorithm for classification and regression tasks. They construct a tree-like structure where each internal node represents a decision based on a feature, and each leaf node represents the final prediction or outcome. Decision trees are interpretable, easy to understand, and can handle both categorical and numerical data.

### Gradient Boosting
Gradient boosting is an ensemble learning technique that combines multiple weak learners, typically decision trees, to create a strong predictive model. It works by iteratively training models that correct the mistakes of the previous models, leading to a highly accurate and robust model. Gradient boosting is known for its ability to handle complex relationships and improve overall performance.

### Random Forest
Random forest is another ensemble learning method that combines multiple decision trees to form a more accurate and stable prediction. It creates an ensemble by training each decision tree on a random subset of the data and using the majority vote or averaging for predictions. Random forest is effective in handling high-dimensional data and mitigating overfitting.

## [[Applications]]
